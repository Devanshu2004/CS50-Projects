# Traffic

#### Video Demo: https://youtu.be/zXXy5fa-QtM
#### Description:

Firstly I tried using Sigmoid activation and it took more time than expected and the accuracy I got was less as compared to what I got using Relu activation, which was about 97.14% and the accuracy that I got using sigmoid activation was about 92%.

While using the Relu activation, during the first Epoch accuracy was about 37.54% and then it gradually increased to 75.9% in the second Epoch and finally in the training data set the accuracy reached till 96.05% and the testing set accuracy was 97.14%.

In the convolution I am using 3x3 grids and for pooling a 2x2. I have added two hidden layers, the first one having 512 nurons and second one having 256 nurons, both having activation function as "Relu". In the hidden layers I have inserted a dropout for the 40% of the time so that the AI learns better.